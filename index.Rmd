---
title: "R Lightning Talk! \\@DSC 2015"
subtitle: "Taste the power of Rcpp"
author: 
    name: "Kyle Chung"
    affiliation: "Trend Micro, Data Scientist"
    email: "kylechun9@gmail.com"
date: '`r format(Sys.time(), "%d %B, %Y")`<br><br><a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="img/by-nc-sa-4.0-88x31.png" /></a>'
output: html_document
bibliography: bibliography.bib
notice: |
    @Hornik:BuchtaLZeileis:2009
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=FALSE)
options(width=120)
```

----------------------
### What exactly is R?

+ It's a program written in R, C++, and Fortran.
+ Let's take a look at its source files.

```{r file_count}
library(data.table)
library(stringr)

# download R source code if you dont have it yet
# download.file("http://cran.csie.ntu.edu.tw/src/base/R-3/R-3.2.1.tar.gz", "R-3.2.1.tar.gz")
# untar("R-3.2.1.tar.gz")

# get all file extensions in R source code
all_src_files <- list.files("./R-3.2.1/src/", recursive=TRUE, full.names=TRUE)
all_ext <- str_extract(all_src_files, "\\.[a-zA-Z]+$")
file_counts <- as.data.table(sort(table(all_ext), decreasing=TRUE), keep.rownames=TRUE)
``` 


### Distribution of source file counts in R

```{r plot_file_count, message=FALSE}
library(wordcloud)
par(mar=c(0, 0, 0, 0))
wordcloud(file_counts$V1, file_counts$V2, min.freq=1, scale=c(7,1), rot.per=.35, random.order=F)
library(ggvis)
file_counts[1:10] %>% ggvis(~reorder(V1, -V2), ~V2) %>% layer_bars() %>%
    add_axis('x', title="Source Type") %>% add_axis('y', title="File Count")
``` 

+ Top-ten file types:
    + Rd: document file
    + **.R: R source**, of course
    + **.C: C source**
    + .mo: binary data file
    + .po: gettext file, about programming language translation
    + **.h: header file for C**
    + .afm: font file
    + .in: usually template config file for some macro preprocessor
    + .win: same above, but for Windows
    + **.f: Fortran source**


```{r source_file_count, cache=TRUE}
# now focus on only source type files
src_ext <- c('c', 'h', 'R', 'f')
srcfiles <- list()
for ( ext in src_ext )
    srcfiles[[ext]] <- grep(sprintf("\\.%s$", ext), all_src_files, value=TRUE)
ext_dist <- sapply(srcfiles, length)
round(ext_dist / sum(ext_dist), 2)
```

As you can see, **R is heavily writen in C.** Indeed, most primitive functions are written in C code. Vectorization is implemented in C code. The speedy part of R is written in C. (And that's why it is speedy.)

For example, ``+`` (the add function) is a primitive function written in C, vectorized.
```{r print_add}
`+` # no R source code printed cause it is written in C
```

In addition to distribution of *number of files* in R source, we can take a further look into the distribution of *number of lines* in each source type. 

```{r line_count, cache=TRUE}
# unix only
wc_res <- sapply(srcfiles, function(x) 
    sum(as.integer(sapply(sprintf("wc -l < %s", x), system, intern=TRUE))))
round(wc_res / sum(wc_res), 2)
```


### Distribution of source line counts in R

```{r plot_line_count}
line_counts <- as.data.table(wc_res, keep.rownames=TRUE)
line_counts %>% ggvis(~rn, ~wc_res) %>% layer_bars() %>%
    add_axis('x', title="Source Type") %>% add_axis('y', title="Line Count", title_offset=60)
```

Finally, we've found that **only `r paste0(round(100 * wc_res['R'] / sum(wc_res), 2), '%')` of R source lines is written in R, and `r paste0(round(100 * wc_res['c'] / sum(wc_res), 2), '%')` of it is written by C, and `r paste0(round(100 * wc_res['f'] / sum(wc_res), 2), '%')` Fortran.**


### When R is not satisfying your need for speed...

+ It's simple. We can write R function in C/C++, which is usually above 10X faster than the equivalent of native R code. 
+ Okay. So **why not just use C/C++** for everything? 
    + cause your *programming time* will explode to blow up all your potential performance gain, well, provided that you really can finish the analytic job purely in C++.
+ This kind of high-level / low-level trade-off is happening in any modern analytic tools.
    + but at least you are capable of trading for such gain in R, which is a very flexible language for analytics.
    + for some other tools you simply CAN'T do anything to speed up your performance.


### But writing C/C++ for R is painful
+ The traditional R API to C/C++ is talking like alien.


### Here comes `Rcpp`

+ What is `Rcpp`?
    + A library for R [@Eddelbuettel:Francois:2011:JSSOBK:v40i08] to build a more manageble API for integrating R and C++, so that users are more easy to write function in C++ to speed up their R program.
    + It provides *syntax sugar* so that the coding is less like C++ programing and more like R programing. Indeed it becomes something in-between.
+ Usefule resources:
    + [Rcpp Github page](https://github.com/RcppCore/Rcpp)
    + [Rcpp website](http://www.rcpp.org)


### Example 1: n-gram generation

Imagin you are dealing with hexdump of files and planning to use its n-gram frequency feature. Your input data may look like this: 

```{r ex1:print_hexdump}
samplefile <- "index.html"
system(sprintf("xxd -g1 %s | head", samplefile), intern=TRUE) # unix only
```

We firstly tidy the input to be a long character sequence...

```{r ex1:process_hexdump}
hexdumplines <- system(sprintf("xxd -g1 %s | cut -d \" \" -f 2-17", samplefile), intern=TRUE) # unix only
hexdump <- unlist(strsplit(paste(hexdumplines, collapse=' '), ' '))
hexdump <- hexdump[hexdump != '']
str(hexdump)
```

Then what? Try looping in native R code, which is rather straight forward.

```{r ex1:r_solution, cache=TRUE}
# first trial: use native R loop (could be terrible for large-scale processing...)
generateNgramFreqR <- function(x, n) {
    len <- length(x) - (n - 1)
    out <- character(len) # pre-allocate size, very important
    for ( i in 1:len ) {
        out[i] <- x[i]
        if ( n > 1 )
            for ( j in 1:(n-1) )
                out[i] <- paste0(out[i], x[i+j])
    }
    unlist(as.list(sort(table(out), decreasing=TRUE)))
}
system.time(r_res <- generateNgramFreqR(hexdump, 2))
r_res[1:10]
```

Or we can use some external library that focuses on text mining, for example, use `tau` [@Buchta:Hornik:Feinerer:Meyer:2015].

```{r ex1:tau_solution, cache=TRUE, message=FALSE}
# second trial: use external library
generateNgramFreqTau <- function(x, n) {
    require(tau)
    tau_res <- textcnt(x, n=n, method="string", split="[[:space:]]+")
    names(tau_res) <- gsub(' ', '', names(tau_res))
    sort(tau_res, decreasing=TRUE)
}
system.time(tau_res <- generateNgramFreqTau(hexdump, 2))
tau_res[1:10]

# check if the results are eqaul
identical(tau_res, r_res)
```

But the performance of the above two solutions are both not satisfying when it comes to large-scale problem. Think about dealing with hex-dump strings from tens of thousands of files, we will need more efficient solution.

Sometimes to make things better all you have to do is to find ANOTHER package that is more powerful over the probelm at hand. But still sometimes you just don't have much to choose. In this very case of generating n-grams, though there is another well-known general text mining pacakge `tm` [@Feinerer:Hornik:2015; @Feinerer:Hornik:Meyer:2008] available, it is not suitable to serve as a faster solution to our problem. Indeed, it is prohibitively slow.

```{r ex1:tm_solution, cache=TRUE, message=FALSE}
# third trial: use another external library, hopefully faster? (Nope.)
generateNgramFreqTm <- function(x, n) {
    require(tm)
    op <- options(mc.cores=NULL) 
    options(mc.cores=1)
    on.exit(options(op))
    
    ngramTokenizer <- function(x) RWeka::NGramTokenizer(x, RWeka::Weka_control(min=2, max=2))
    corp <- Corpus(VectorSource(paste(x, collapse=' ')))
    tdm <- TermDocumentMatrix(corp, control=list(tokenize=ngramTokenizer))
    res <- rowSums(as.matrix(tdm))
    names(res) <- gsub(' ', '', names(res))
    sort(res, decreasing=TRUE)
}

# full-size input not run, too slow
length(hexdumplines)
system.time(tm_res <- generateNgramFreqTm(hexdumplines[1:10000], 2))
tm_res[1:10]
```

Let's try `Rcpp` to see what we could further achieve.

```{r ex1:cpp_solution, cache=TRUE}
# final trial: use Rcpp to write the function in C++
library(Rcpp)
cppFunction('
    CharacterVector generateNgramFreq (CharacterVector hexvector, int ngram) {
        int len = hexvector.size() - (ngram - 1);
        CharacterVector out(len);
        for (int i = 0; i < len; i++) {
            out(i) = hexvector[i];
            for (int j = 1; j < ngram; j++) {
                out(i) += hexvector[i+j];
            }
        }
        return out;
    }')
generateNgramFreq
generateNgramFreqCpp <- function(x, n)
    unlist(as.list(sort(table(generateNgramFreq(x, n)), decreasing=TRUE)))
system.time(cpp_res <- generateNgramFreqCpp(hexdump, 2))
cpp_res[1:10]
identical(cpp_res, r_res)
```

Now it's time to benchmark these solutions to see the performance difference. You'll see that the `Rcpp` solution is above 10X faster than its alternatives. Depends on the scale of your problem such gap could be further enlarged.

```{r ex1:benchmark, cache=TRUE}
library(microbenchmark)
microbenchmark(generateNgramFreqR(hexdump, 2),
               generateNgramFreqTau(hexdump, 2),
               generateNgramFreqCpp(hexdump, 2), times=10)
```

+ Interested? See the [Microsoft malware classification challenge](https://www.kaggle.com/c/malware-classification)


### Example 2: moving-window computing

```{r, cache=TRUE}
cppFunction('
    NumericVector movingCount(NumericVector tindex, NumericVector dataT, NumericVector dataC, int stepsize) {
        int n = tindex.size();
        NumericVector out(n);
        for (int i = 0; i < n; i++) {
            int begin = tindex[i];
            int end = begin + stepsize;
            NumericVector cnts = dataC[(dataT >= begin) & (dataT < end)];
            out(i) = sum(cnts);
        }
        return out;
    }')
```


### Example 3: ad-hoc data.frame operation

```{r, cache=TRUE}
cppFunction('
    NumericVector countAnythingBoundedByTime (NumericVector first, NumericVector last) {
        int len = first.size();
        NumericVector out(len);
        for (int i = 0; i < len; i++) {
            out(i) = 0;
            for (int j = 0; j < len; j++) {
                if (first(j) <= last(i)) {
                    out(i) += 1;
                }
            }
        }
        return out;
    }')
```


### References
